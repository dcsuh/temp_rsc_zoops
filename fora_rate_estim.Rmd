---
title: "estimate foraging rate"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Issues

- Getting a lot of warning messages the lsoda() (i.e. the numerical integrator) about "Warning: an excessive amount of work (> maxsteps ) was done, but integration was not successful - increase maxstepsWarning: Returning early. Results are accurate, as far as they go". I am pretty unsure of how to debug lsoda

- optimizer seems relatively sensitive to starting conditions? I get different results if I change the starting condition by a power of 10. (e.g. f = 0.03 instead of 0.003)

- likelihood of the more complex models is worse than the most basic model. Not sure why this is the case. The model results look like they fit the data better for the more complex models.


```{r packages, message=FALSE}
library(here)
library(tidyverse)
library(deSolve)
library(magrittr)
library(bbmle)
library(latex2exp)
library(AICcmodavg)
```

```{r, set theme}
theme_set(theme_minimal())
set.seed(8878896)

kelvin <- 273.15
```


The purpose of this document is to estimate foraging rate using maximum likelihood estimation under different model formulations. In its simplest form, foraging rate is a constant unaffected by any external conditions. However, we hypothesize that temperature and resource concentration will influence foraging rate. Under different resource conditions, we expect foraging rate to change with changes in resource concentration following a type II functional response. Under a type II functional response, foraging rate increases non-linearly with resource concentration and will asymptote at a sufficiently high resource concentration. Mechanistically, this is due to the inclusion of handling time. At a certain point, feeding rate is constrained by handling time, even if there is a seemingly endless amount of food available. Temperature is also expected to change foraging rate behavior. Temperature may increase encounter rates with food and cause the consumer to forage at a higher rate. We attempt to incorporate the effects of temperature using an Arrhenius equation, an equation typically used to model temperature affects on reaction rates. 

Overall, we will develop four models that can be used to describe foraging rate. Our independent model assumes no affects of temperature or resource conditions. Our resource-dependent model assumes a type II functional response. Our temperature-dependent model incorporates an affect of temperature using an Arrhenius equation. Our full model incoporates effects of both resource and temperature.

We will use each of these models to develop parameter estimates for foraging rate. Each model is an equation and can be used to generate data. We generate data for a range of parameter estimates and measure the likelihood of these simulated data given our actual observed data. We choose the parameter estimate under each model that maximizes the likelihood. We choose the model with the highest likelihood. 


We use the raw data so we will have to do a little bit of housekeeping before we can actually use these data.
```{r, real data}
data <- readRDS(here("processed_data","foraging_raw.rds"))
data_summ <- readRDS(here("processed_data", "foraging.rds"))

data %<>% mutate(treatment_ID = paste(temp, resource, sep = "_"))

data %<>% mutate(resource_tot = resource*vol) #resource_tot is the total amount of resources in our tube. resource concentration * total volume of tube 

treatment_IDs <- unique(data$treatment_ID)

```



# Model 0 - Basic model


Let's try this with the same units as the foraging rate data. ml/min. So R should be mgC/mL and S should be hosts/mL
$$
\frac{dR}{dt} = -fRS \\
$$

```{r, m0_num_sol}
m0_num_sol <- function(t, x, params){
  R <- x[1]
  with(as.list(params),{
    dR <- -f*R*S
    res <- c(dR)
    list(res)}
    )
}

maxTime <- 0.4
times <- seq(0,maxTime,by=0.01)

params <- c(f=0.0003,
            S=1/15*1000)

xstart <- c(R=0.1)

output <- as.data.frame(lsoda(xstart, times, m0_num_sol, params))

r_end <- slice_max(output, time)[,2]
```

```{r, m0 output}
output %>% ggplot(., aes(x=time, y=R)) + geom_line()
```

Let's see what kind of values we get for our entire range of treatment conditions

```{r, m0 params}
m0_params <- data_summ %>% select(temp, resource, rate_mean, rate_len_mean, conc_mean, amt_mean, time_mean, length_mean, ID)

for (i in 1:nrow(m0_params)){
  maxTime <- m0_params$time_mean[i]
  times <- seq(0,maxTime,by=0.01)
  
  params <- c(f=m0_params$rate_mean[i],
              S=1/15*1000)
  
  xstart <- c(R=m0_params$resource[i])
  
  output <- as.data.frame(lsoda(xstart, times, m0_num_sol, params))
  
  r_end <- slice_max(output, time)[,2]
  
  m0_params$final_conc[i] <- r_end
}

m0_params %<>% mutate(final_amt = final_conc*15)
```

```{r, compare m0 output to data}
ggplot(m0_params, aes(x=ID, y=(final_conc/conc_mean))) + geom_point() + labs(x="treatment", y=TeX("$\\frac{theoretical}{actual}"))
ggplot(m0_params, aes(x=ID, y=(final_amt/amt_mean))) + geom_point() + labs(x="treatment", y=TeX("$\\frac{theoretical}{actual}"))
```

The basic model can recreate the data fairly well but is insufficient for certain treatments. Let's try to see if the optimizer can provide us with the same foraging rate estimates that we know should be fairly accurate under certain treatments.

```{r, m0_ll}
m0_ll <- function(f){
  data <- tmp
  resid_df <- tibble(resid=0, data$ID)
    for (i in 1:nrow(data)) {
    maxTime <- data$time[i] #sets endpoint time
    times <- seq(0, maxTime, by=0.01)
    params <- c(f=f,
                S=1/15*1000)
    xstart <- c(R=data$resource[i]) #absolute resource amount in tube
    output <- as.data.frame(lsoda(xstart, times, m0_num_sol, params))
    
    r_end <- slice_max(output, time)[,2]
    
    r_end <- ifelse(r_end<0, 0, r_end) #ensure that final resource concentration is never negative
      
    resid_df$resid[i] <- r_end - data$amt_rem[i]
    }
  
    nll <- dnorm(resid_df$resid, 
                 mean = 0, 
                 sd = sd(resid_df$resid, na.rm = T), 
                 log = T)
    -sum(nll, na.rm = T)
}
```

Note: I was trying to set the mean for dnorm() to the mean of the residuals but this would cause the optimizer to get stuck on nonsense values. E.g. very high values of f would result in the residuals all being negative. The likelihood of this distribution using the mean of the negative residuals would sometimes be lower even though having all negative residuals means that the parameter estimate is very off. The mean should be set to zero because the residuals should center around zero if the estimate is close to the actual. 

```{r, check m0 resid}
#resid_df %>% ggplot(.,aes(x=resid)) + geom_histogram(binwidth=0.005)
```

Let's run the optimizer through a loop for each treatment


```{r, m0_mle fits by treatment, eval = FALSE}
start_time <- Sys.time()
m0_f_fits <- list()
for(j in 1:nrow(m0_params)){
  tmp <- data %>% filter(treatment_ID==m0_params$ID[j])
  m0_f_fits[[j]]<-mle2(m0_ll,
                    start=list(f=m0_params$rate_mean[j]),
                    method = "Brent", # I use Brent because we only have on parameter
                    lower = 0,
                    upper = 0.1)
}
# m0_f_fit <- mle2(m0_ll,
#                 start=list(f=0.003),
#                 method = "Brent", # I use Brent because we only have on parameter
#                 lower = 0,
#                 upper = 1000)
end_time <- Sys.time()
m0_runtime <- end_time - start_time
m0_runtime

#opt1 <- optim(fn=m1_ll, par=c(30), method = "Brent", lower = 0, upper = 100) #this should be the same as mle2() since mle2() is a wrapper for optim()

saveRDS(m0_f_fits, file = here("mle","m0_f_fits.rds"))
```




```{r, m1_f_fit by treatment results}
m0_f_fits <- readRDS(here("mle", "m0_f_fits.rds"))

for(i in 1:9){
  print(paste("", i))
  print(as.numeric(m0_params$rate_mean[i]))
  print(as.numeric(m0_f_fits[[i]]@coef))
}
```


These values seem like they could make sense. 4, 7, 8, and 9 are the treatments that had the most discrepancy between the foraging rate estimated from the data and from the simulation.

Can I rerun the model results using these new estimates and see if it better predicts the data?


```{r, m0_mle, eval = FALSE}
start_time <- Sys.time()
tmp <- data
m0_f_fit <- mle2(m0_ll,
                start=list(f=0.003),
                method = "Brent", # I use Brent because we only have on parameter
                lower = 0,
                upper = 0.01)
end_time <- Sys.time()
m0_runtime <- end_time - start_time
m0_runtime

#opt1 <- optim(fn=m1_ll, par=c(30), method = "Brent", lower = 0, upper = 100) #this should be the same as mle2() since mle2() is a wrapper for optim()

saveRDS(m0_f_fit, file = here("mle","m0_f_fit.rds"))

```

```{r, m0_f_fit output}
summary(readRDS(here("mle","m0_f_fit.rds")))
```




# Model 1 - Independent model

Model 1 will also serve as an example for the more detailed methods used.

First, we can write out our equations. To do this, we should think about our experiment. In our foraging rate assay, we measured the change in resource (algae) concentration in a tube over time after adding a single daphnia to the tube. So we can write an equation to model the change in resource concentration over time.

We use a first-order linear differential equation to represent resource concentration because we expect the rate of change to depend on the amount of resources available. Normally, the rate of change of the resource (our state variable) might also include an expression for growth (usually logistic), but because we conducted this experiment in the dark we can assume that no growth is occurring and can omit omit that expression. We are left with just the loss term (how much is being foraged). Our loss term should also be a per-capita term and depend on the number of individuals foraging, but in our experiment there is only a single daphnia ever foraging per assay.
$$
\begin{equation}
f(R) = R + \frac{dR}{dt} \\
\frac{dR}{dt} = -fRS \\
\frac{dR}{dt} = -fR \\
\end{equation}
$$

Foraging rate is also expected to depend on the size of the individual. We incorporate length as a factor with a power coefficient. We can attempt to fit gamma or just assume a reasonable value. In other papers, a power coefficient of 2 is used. The idea here (I think) is that the surface area of the daphnia is what is relevant for foraging. Squaring the length gives us the surface area of a square which is close enough to the surface area of a daphnia?

$$
\frac{dR}{dt} = -L^{\gamma} fRS \\
$$

Now that we have our full equation, we can use this to estimate the amount of algae after a given amount of time. We can do this by numerically solving our differential equation and seeing how much algae remains after a given amount of time. We can also solve the differential equation by writing a function that represents the change of algae over time. We'll do both of these here.

```{r, m1_num_sol}
m1_num_sol <- function(t, x, params){
  R <- x[1]
  with(as.list(params),{
    dR <- -(length^gamma)*f*R*S
    res <- c(dR)
    list(res)}
    )
}

maxTime <- 0.4
times <- seq(0,maxTime,by=0.01)

params <- c(f=0.001,
            length=1,
            gamma=2,
            S=1/15*1000)

xstart <- c(R=1)

output <- as.data.frame(lsoda(xstart, times, m1_num_sol, params))

r_end <- slice_max(output, time)[,2]
```

```{r, m1 output}
output %>% ggplot(., aes(x=time, y=R)) + geom_line()
```


We can use this chunk of code to get the solution for our model given any set of starting conditions and parameter values. Using our data, we can determine what set of parameter values will allow the model to best fit our data. In our data, we have information about each daphnia (i.e. length) and how much they foraged over a certain amount of time.


Using the actual data, we can go ahead and try out estimating f.

pseudocode:
write a loop that goes through each treatment. For each treatment, set the conditions of the solver (i.e. resource concentration; in future cases, this will also include temperature). Before we do this, we need to write a function that will pop out the negative log-likelihood. To do this, we will need to use the solver to give us the endpoint resource concentration given a certain foraging rate. We use a function like dnorm() to calculate the likelihood of of observing data, given a certain probability. In this case, the probability is set from the result from the solver. Together, the result from the solver and the data are used to give us the likelihood and simple maths turns the likelihood into the negative log-likelihood (i.e. taking the log of the likelihood and making it negative). The optimizer (mle2() is what we use) takes this likelihood function and then moves through parameter space to find the parameter value that minimizes the sum of the negative log-likelihood of all the data. 


I saw two methods for using the solver. One was to average across each treatment, run the solver for these conditions, and then use those values to calculate residuals which are fed into dnorm(). The other way was to run the solver for each observation and use the unique conditions for each individual to set the conditions for the solver (length and time of assay is different for individuals within a treatment). I then get the residual for each observation and feed that to dnorm(). I went with the second option to start because it seems the most accurate to me. The downside is that it will likely take significantly longer than the first option since the solver is being much more often.


```{r, m1_ll}
m1_ll <- function(f){
  data <- tmp
  resid_df <- tibble(resid=0, data$ID)
    for (i in 1:nrow(data)) {
    maxTime <- data$time[i] #sets endpoint time
    times <- seq(0, maxTime, by=0.01)
    params <- c(f=f,
                length=data$mm[i], #length measurement for each individual
                gamma=2,
                S=1/15*1000)
    xstart <- c(R=data$resource[i]) #starting resource concentration
    output <- as.data.frame(lsoda(xstart, times, m1_num_sol, params))
    
    r_end <- slice_max(output, time)[,2]
      
    resid_df$resid[i] <- r_end - data$amt_rem[i] #amt_rem is final resource conc.
    }
  
    nll <- dnorm(resid_df$resid, 
                 mean = 0, 
                 sd = sd(resid_df$resid, na.rm = T), 
                 log = T)
    -sum(nll, na.rm = T)
}
```



This iteration is the same as m1_ll but also estimates gamma
```{r, m1_ll_alt2}
m1_ll_alt <- function(f, gamma){
  resid_df <- tibble(resid=0, data$ID)
    for (i in 1:nrow(data)) {
    maxTime <- data$time[i]
    times <- seq(0, maxTime, by=0.01)
    params <- c(f=f,
                length=data$mm[i],
                gamma=gamma,
                S=1/15*1000)
    xstart <- c(R=data$resource[i])
    output <- as.data.frame(lsoda(xstart, times, m1_num_sol, params))

    r_end <- slice_max(output, time)[,2]

    resid_df$resid[i] <- r_end - data$amt_rem[i]
    }

    nll <- dnorm(resid_df$resid,
                 mean = 0,
                 sd = sd(resid_df$resid, na.rm = T),
                 log = T)
    -sum(nll, na.rm = T)
}
```


```{r, check m1 resid,eval=F}
#resid_df %>% ggplot(.,aes(x=resid)) + geom_histogram(binwidth=0.25)
```

Let's try this treatment by treatment

```{r, m1_f_fit by treatment, eval = F}
m1_f_fits <- list()
for(j in 1:nrow(m0_params)){
  tmp <- data %>% filter(treatment_ID==m0_params$ID[j])
  m1_f_fits[[j]]<-mle2(m1_ll,
                    start=list(f=m0_params$rate_len_mean[j]),
                    method = "Brent", # I use Brent because we only have one parameter
                    lower = 0,
                    upper = 0.1)
}
saveRDS(m1_f_fits, file = here("mle","m1_f_fits.rds"))
```


```{r, m1_f_fits by treatment}
m1_f_fits <- readRDS(here("mle", "m1_f_fits.rds"))

for(i in 1:9){
  print(paste("", i))
  print(as.numeric(m0_params$rate_len_mean[i]))
  print(as.numeric(m1_f_fits[[i]]@coef))
}
```


Now we use the optimizer
```{r, m1_mle, eval = FALSE}
start_time <- Sys.time()
tmp <- data
m1_f_fit <- mle2(m1_ll,
                start=list(f=0.001),
                method = "Brent", # I use Brent because we only have one parameter
                lower = 0,
                upper = 0.1)
end_time <- Sys.time()
m1_runtime <- end_time - start_time
m1_runtime
#opt1 <- optim(fn=m1_ll, par=c(30), method = "Brent", lower = 0, upper = 100) #this should be the same as mle2() since mle2() is a wrapper for optim()
saveRDS(m1_f_fit, file = here("mle","m1_f_fit.rds"))
```

```{r, m1_f_fit}
summary(readRDS(here("mle","m1_f_fit.rds")))
```


This version is for estimating f and gamma
```{r, m1_mle_alt, eval = FALSE}
start_time <- Sys.time()
m1_f_fit_alt <- mle2(m1_ll_alt,
                start=list(f=0.003,
                           gamma=2),
                method = "Nelder-Mead")
end_time <- Sys.time()
m1_alt_runtime <- end_time - start_time
m1_alt_runtime

saveRDS(m1_f_fit_alt, file = here("mle","m1_f_fit_alt.rds"))
```

```{r, m1_f_fit_alt}
summary(readRDS(here("mle", "m1_f_fit_alt.rds")))
```


Note: I was having issues with the solver getting stuck when using mle(). I could get mle2() and optim() to work though so not sure what the issue is.



# Model 2 - Temperature-dependent model

I try to incorporate temperature here using the Arrhenius equation as a rate coefficient of foraging rate.


$$
\frac{dR}{dt} = -L^{\gamma}fe^{T_{A}(1/T_{R}-1/T)} R S \\
$$

Let's estimate a reasonable value for the Arrhenius coefficient given the actual data we have. Using the arrhenius equation there should, in theory, be a reasonable value for the Arr coefficient that will allow us to have one value of f that will depend on temperature. So if the reference temperature is 15 degrees, then we will have one value of f that we observed. The value of f that we observed at 20 degrees is different than our observed value of f at 15 degrees, but we should be able to make these terms equivalent using the Arrhenius equation.

$$
-L^{\gamma}f R S \; at\; reference\; temperature = -L^{\gamma}fe^{T_{A}(1/T_{R}-1/T)} R S  \\
(observed\; f) = (reference\; f)*e^{T_{A}(1/T_{R}-1/T)}  \\
\frac{observed\; f}{reference\; f} = e^{T_{A}(1/T_{R}-1/T)} \\
ln(\frac{observed\; f}{reference\; f}) = T_{A}(1/T_{R}-1/T) \\
\frac{ln(\frac{observed\; f}{reference\; f})}{(1/T_{R}-1/T)} = T_{A} \\
$$


Let's look at what the Arrhenius coefficient might be at different resource conditions
```{r, break up data by resource}
data_0.1 <- data_summ %>% filter(resource==0.1)
data_0.5 <- data_summ %>% filter(resource==0.5)
data_1 <- data_summ %>% filter(resource==1)
```

```{r, make get_arr}
get_arr <- function(data){
  if(data$rate_mean[1]<0){
    f_ref <- 0.000000001
  } else{
    f_ref <- data$rate_mean[1]
  }
  dat <- c()
  for (i in 2:3){
      dat[i-1] <- ((log(data$rate_mean[i]/f_ref))/((1/(15+kelvin))-(1/(data$temp[i]+kelvin))))
  }
  return(mean(dat))
}
```


```{r, get arr by resource}
arr_0.1 <- get_arr(data_0.1)
arr_0.5 <- get_arr(data_0.5)
arr_1 <- get_arr(data_1)
```



```{r, m2_num_sol}
m2_num_sol <- function(t, x, params){
  R <- x[1]
  with(as.list(params),{
    dR <- -(length^gamma)*f*exp(arr_t*(1/ref_t - 1/temp))*R*S
    res <- c(dR)
    list(res)}
    )
}

maxTime <- 0.4
times <- seq(0,maxTime,by=0.005)

params <- c(f=0.005,
            length=1,
            gamma=2,
            arr_t=10000, #arrhenius coefficient
            ref_t=15+kelvin, #reference temperature
            temp=25+kelvin,
            S=1/15*1000) #treatment temperature

xstart <- c(R=1)

output <- as.data.frame(lsoda(xstart, times, m2_num_sol, params))

r_end <- slice_max(output, time)[,2]
```

```{r, m2 output}
output %>% ggplot(., aes(x=time, y=R)) + geom_line()
```



Let's loop through some example parameters to make sure that the temperature-dependent model is behaving the way we expect it to.

```{r, m2 check, eval = F}
n <- 11

param_set <- tibble(f = seq(0, 0.01, length.out = n),
                    arr_t = seq(0, 10000, length.out = n),
                    temp = seq(5,25, length.out = n),
                    R = seq(0, 1, length.out = n))

dat <- tibble(f= NA,
              arr_t = NA,
              temp = NA,
              R = NA,
              end = NA)

for (i in 1:n){
  f <- param_set$f[i]
  for (j in 1:n){
    arr_t <- param_set$arr_t[j]
    for (k in 1:n){
      temp <- param_set$temp[k]
      for (m in 1:n){
        R <- param_set$R[m]
        maxTime <- 0.4
        times <- seq(0,maxTime,by=0.01)
        params <- c(f=f,
                    length=0.9,
                    gamma=2,
                    arr_t=arr_t, #arrhenius coefficient
                    ref_t=15+kelvin, #reference temperature
                    temp=temp+kelvin,
                    S=1/15*1000) #treatment temperature
        xstart <- c(R=R)
        output <- as.data.frame(lsoda(xstart, times, m2_num_sol, params))
        r_end <- slice_max(output, time)[,2]
        dat %<>% add_row(f = f,
                         arr_t = arr_t,
                         temp = temp+kelvin,
                         R = R,
                         end = r_end)
      }
    }
  }
}

saveRDS(dat, here("mle", "m2_check.rds"))
```


```{r, m2 check plot}
m2_check <- readRDS(here("mle", "m2_check.rds"))

m2_check %>% filter(R==1 & f>0) %>% ggplot(., aes(x = arr_t, y = temp, fill = 1-end/R)) + geom_tile() + facet_wrap(~f) + scale_fill_viridis_c()
```

$$
(observed\; f) = (reference\; f)*e^{T_{A}(1/T_{R}-1/T)}  \\
$$

```{r}
m2_check %<>% mutate(f_adj = f*exp(arr_t*((1/(15+kelvin))-(1/(temp)))),
                     arr_coef = exp(arr_t*((1/(15+kelvin))-(1/(temp)))))
```

```{r}
m2_check %>% filter(R==1 & f > 0) %>% ggplot(., aes(x = arr_t, y = temp, fill = f_adj)) + geom_tile() + scale_fill_viridis_c() + facet_wrap(~f)
```



```{r, m2_ll}
m2_ll <- function(f, arr_t){
  data <- tmp
  resid_df <- tibble(resid=0, data$ID)
    for (i in 1:nrow(data)) {
    maxTime <- data$time[i]
    times <- seq(0, maxTime, by=0.001)
    params <- c(f=f,
                length=data$mm[i],
                gamma=2,
                arr_t=arr_t,
                ref_t=15+kelvin,
                temp=data$temp[i]+kelvin,
                S=1/15*1000)
    xstart <- c(R=data$resource[i])

    output <- as.data.frame(lsoda(xstart, times, m2_num_sol, params))
    
    r_end <- slice_max(output, time)[,2]
      
    resid_df$resid[i] <- r_end - data$amt_rem[i]

    }
  
    nll <- dnorm(resid_df$resid, 
                 mean = 0, 
                 sd = sd(resid_df$resid, na.rm = T), 
                 log = T)
    -sum(nll, na.rm = T)
}
```



```{r, combine data}
arr_data <- c(arr_0.1,arr_0.5,arr_1)
```


```{r, m2_f_fit for each resource level, eval = F}
start_time <- Sys.time()
m2_f_fits <- list()
for(j in 1:(length(unique(data$resource)))){
  tmp <- data %>% filter(resource==unique(data$resource)[j])
  m2_f_fits[j] <- mle2(m2_ll,
                       start=list(f=0.001,
                                  arr_t=1000),
                       method = "BFGS")
}
end_time <- Sys.time()
end_time - start_time


saveRDS(m2_f_fits, file = here("mle","m2_f_fits.rds"))

```

```{r, m2_f_fits by resource}
m2_f_fits
```



```{r, m2_mle, eval = FALSE}
start_time <- Sys.time()
tmp <- data
m2_f_fit <- mle2(m2_ll,
                start=list(f=0.005,
                           arr_t=10000),
                method = "L-BFGS-B",
                lower = c(0, 0),
                upper = c(Inf, 100000))
end_time <- Sys.time()
m2_runtime <- end_time - start_time
m2_runtime

saveRDS(m2_f_fit, file = here("mle","m2_f_fit.rds"))
```

```{r, m2_f_fit}
summary(readRDS(here("mle", "m2_f_fit.rds")))
```



# Model 3 - Resource-dependent model

I try to incorporate resource-dependence by using an equation for the type II functional response.



$$
\frac{dR}{dt} = - \frac{fL^{\gamma}R}{1 + fL^{\gamma}hR}S \\
$$




```{r, m3_num_sol}
m3_num_sol <- function(t, x, params){
  R <- x[1]
  with(as.list(params),{
    dR <- -((f*length^gamma*R)/(1+f*length^gamma*h*R))*S
    res <- c(dR)
    list(res)}
    )
}

maxTime <- 1000
times <- seq(0,maxTime,by=0.5)

params <- c(f=0.01,
            length=1,
            gamma=2,
            h=2,
            S=1/15)

xstart <- c(R=0.1)

output <- as.data.frame(lsoda(xstart, times, m3_num_sol, params))

r_end <- slice_max(output, time)[,2]
```


```{r, m3 output}
output %>% ggplot(., aes(x=time, y=R)) + geom_line()
```

What are some reasonable estimates for h?

$$
observed\; f R S= \frac{fL^{\gamma}RS}{1 + fL^{\gamma}hR} \\
observed\; f  = \frac{fL^{\gamma}}{1 + fL^{\gamma}hR} \\
1 + fL^{\gamma}hR\;(observed\; f) = fL^{\gamma} \\
fL^{\gamma}hR\;(observed\; f) = fL^{\gamma} -1 \\
h = \frac{fL^{\gamma} - 1}{fL^{\gamma}R\;(observed\; f)}\\
$$


```{r, break up data by temp}
data_15 <- data_summ %>% filter(temp==15)
data_20 <- data_summ %>% filter(temp==20)
data_25 <- data_summ %>% filter(temp==25)
```

```{r, make get_h}
get_h <- function(data){
  if(data$rate_mean[3]<0){
    f_ref <- 0.000000001
  } else{
    f_ref <- data$rate_mean[3]
  }
  dat <- c()
  for (i in 1:2){
      dat[i] <- ((f_ref*data$resource[i]*(1/15)-1)/f_ref*data$resource[i]*data$rate_mean[i])
  }
  return(mean(dat))
}
```

```{r, get h by temp}
get_h(data_15)
get_h(data_20)
get_h(data_25)
```


Let's see what we get for different values of h and R

```{r, m3 check, eval = FALSE}
n <- 9

param_set <- tibble(f = seq(0.001, 0.1, length.out = n),
                    h = seq(0, 10, length.out = n),
                    R = seq(0.1, 1, length.out = n))

dat <- tibble(f= NA,
              h = NA,
              R = NA,
              end = NA)

for (i in 1:n){
  f <- param_set$f[i]
  for (j in 1:n){
    h <- param_set$h[j]
      for (m in 1:n){
        R <- param_set$R[m]
        maxTime <- 0.5
        times <- seq(0,maxTime,by=0.01)
        params <- c(f=f,
                    length=1,
                    gamma=2,
                    h=h, #arrhenius coefficient
                    S=1/15*1000) #treatment temperature
        xstart <- c(R=R)
        output <- as.data.frame(lsoda(xstart, times, m3_num_sol, params))
        r_end <- slice_max(output, time)[,2]
        dat %<>% add_row(f = f,
                         h = h,
                         R = R,
                         end = r_end)
      }
  }
}


saveRDS(dat, here("mle","m3_check.rds"))
```



Per-capita risk of being eaten

$$
\frac{1}{RS}\frac{dR}{dt} = - \frac{fL^{\gamma}}{1 + fL^{\gamma}hR} \\
$$

```{r, m3 check plot}
m3_check <- readRDS(here("mle", "m3_check.rds"))

m3_check %<>% mutate(attack=f/(1+f*h*R))

m3_check %>% na.omit() %>% ggplot(., aes(x = h, y = R, fill = attack)) + geom_tile() + facet_wrap(~f) + scale_fill_viridis_c()
```




```{r, m3_ll}
m3_ll <- function(f, h){
  data <- tmp
  resid_df <- tibble(resid=0, data$ID)
    for (i in 1:nrow(data)) {
    maxTime <- data$time[i]
    times <- seq(0, maxTime, by=0.01)
    params <- c(f=f,
                length=data$mm[i],
                gamma=2,
                h=h,
                S=1/15*1000) #handling time
    xstart <- c(R=data$resource[i])
    output <- as.data.frame(lsoda(xstart, times, m3_num_sol, params))
    
    r_end <- slice_max(output, time)[,2]
      
    resid_df$resid[i] <- r_end - data$amt_rem[i]
    }
  
    nll <- dnorm(resid_df$resid, 
                 mean = 0, 
                 sd = sd(resid_df$resid, na.rm = T), 
                 log = T)
    -sum(nll, na.rm = T)
}
```


```{r, message=F, warning=FALSE, eval = FALSE}
start_time <- Sys.time()
m3_f_fits <- list()
for(j in 1:length(unique(data$temp))){
  tmp <- data %>% filter(temp==unique(data$temp)[j])
  m3_f_fits[j] <- mle2(m3_ll,
                       start=list(f=0.003,
                                  h=1),
                       method = "Nelder-Mead")
}
end_time <- Sys.time()
end_time - start_time


saveRDS(m3_f_fits, file = here("mle","m3_f_fits.rds"))

```

```{r, get m3_f_fit by temp}
summary(readRDS(here("mle", "m3_f_fits.rds")))
```



```{r, m3_mle, eval = F}
start_time <- Sys.time()
tmp <- data
m3_f_fit <- mle2(m3_ll,
                start=list(f=0.003, h=1),
                method = "Nelder-Mead")
end_time <- Sys.time()
m3_runtime <- end_time-start_time
m3_runtime

saveRDS(m3_f_fit, file = here("mle","m3_f_fit.rds"))
```

```{r, m3_f_fit}
summary(readRDS(here("mle", "m3_f_fit.rds")))
```



# Model 4 - Temperature and resource-dependent model

I try to incorporate both here by just smashing everything together.

$$
\frac{dR}{dt} = -\frac{fe^{T_{A}(1/T_{R}-1/T)} L^{\gamma}R}{1 + fe^{T_{A}(1/T_{R}-1/T)}hL^{\gamma}R} S \\
$$


```{r, m4_num_sol}
m4_num_sol <- function(t, x, params){
  R <- x[1]
  with(as.list(params),{
    dR <- -((f*exp(arr_t*(1/ref_t - 1/temp))*(length^gamma)*R)/(1+f*exp(arr_t*(1/ref_t - 1/temp))*h*(length^gamma)*R))*S
    res <- c(dR)
    list(res)}
    )
}

maxTime <- 500
times <- seq(0,maxTime,by=0.5)

params <- c(f=0.01,
            length=1,
            gamma=2,
            arr_t=100,
            ref_t=15,
            h=10, #handling time
            temp=25,
            S=1/15)

xstart <- c(R=1)

output <- as.data.frame(lsoda(xstart, times, m4_num_sol, params))

r_end <- slice_max(output, time)[,2]
```



```{r, m4 output}
output %>% ggplot(., aes(x=time, y=R)) + geom_line()
```

```{r, m4 check set params}
n <- 10

param_set <- tibble(f = seq(0, 0.01, length.out = n),
                    h = seq(0, 10, length.out = n),
                    arr_t = seq(0, 1000, length.out = n),
                    temp = seq(15,25, length.out = n),
                    R = seq(0, 1, length.out = n))
```


```{r, m4 check, eval = F}
dat <- tibble(f= NA,
              h = NA,
              arr_t = NA,
              temp = NA,
              R = NA,
              end = NA)

for (i in 1:n){
  f <- param_set$f[i]
  for (j in 1:n){
    h <- param_set$h[j]
      for (k in 1:n){
        arr_t <- param_set$arr_t[k]
        for (p in 1:n){
          temp <- param_set$temp[p]
          for (m in 1:n){
          R <- param_set$R[m]
          maxTime <- 600
          times <- seq(0,maxTime,by=0.5)
          params <- c(f=f,
                      length=1,
                      gamma=2,
                      arr_t = arr_t,
                      temp = temp,
                      h=h,
                      ref_t = 15,
                      S=1/15) #treatment temperature
          xstart <- c(R=R)
          output <- as.data.frame(lsoda(xstart, times, m4_num_sol, params))
          r_end <- slice_max(output, time)[,2]
          dat %<>% add_row(f = f,
                           h = h,
                           arr_t = arr_t,
                           temp = temp,
                           R = R,
                           end = r_end)
          }
        }
      }
  }
}

saveRDS(dat, here("mle","m4_check.rds"))
```

```{r, m4 check plot}
m4_check <- readRDS(here("mle", "m4_check.rds"))

x<-9
y<-2

m4_check %>% filter(h == param_set$h[x] & arr_t == param_set$arr_t[y] & f > 0 & R > 0) %>% 
  ggplot(., aes(x=R, y=temp, fill=1-end/R)) + 
  geom_tile() + 
  facet_wrap(~f) + 
  scale_fill_viridis_c()
```



```{r, m4_ll}
m4_ll <- function(f, arr_t, h){
  resid_df <- tibble(resid=0, data$ID)
    for (i in 1:nrow(data)) {
    maxTime <- data$time[i]
    times <- seq(0, maxTime, by=0.005)
    params <- c(f=f,
                length=data$mm[i],
                gamma=2,
                arr_t=arr_t,
                ref_t=15+kelvin,
                temp=data$temp[i]+kelvin,
                h=h,
                S=1/15*1000)
    xstart <- c(R=data$resource[i])
    output <- as.data.frame(lsoda(xstart, times, m4_num_sol, params))
    
    r_end <- slice_max(output, time)[,2]
      
    resid_df$resid[i] <- r_end - data$amt_rem[i]
    }
  
    nll <- dnorm(resid_df$resid, 
                 mean = 0, 
                 sd = sd(resid_df$resid, na.rm = T), 
                 log = T)
    -sum(nll, na.rm = T)
}
```



```{r, m4_mle, eval = F}
start_time <- Sys.time()
m4_f_fit <- mle2(m4_ll,
                start=list(f=0.0001, arr_t=1000, h=10),
                method = "L-BFGS-B",
                lower = c(0, 0, 0),
                upper = c(Inf, Inf, Inf))
end_time <- Sys.time()
m4_runtime <- end_time-start_time
m4_runtime

saveRDS(m4_f_fit, file = here("mle","m4_f_fit.rds"))
```

```{r, m4_f_fit}
summary(readRDS(here("mle","m4_f_fit.rds")))
```

How well do these parameter estimates and this model predict for the data from the experiment?

```{r, check estimates}
m4_f_fit <- readRDS(here("mle","m4_f_fit.rds"))

resid_df <- tibble(resid=0, data$ID, data$treatment_ID, data$resource, data$temp)
f <- m4_f_fit@coef[1]
arr_t <- m4_f_fit@coef[2]
h <- m4_f_fit@coef[3]
for (i in 1:nrow(data)) {
    maxTime <- data$time[i]
    times <- seq(0, maxTime, by=1)
    params <- c(f=as.numeric(f),
                length=data$mm[i],
                gamma=2,
                arr_t=as.numeric(arr_t),
                ref_t=15,
                temp=data$temp[i],
                h=as.numeric(h),
                S=1/15)
    xstart <- c(R=data$resource[i])
    output <- as.data.frame(lsoda(xstart, times, m4_num_sol, params, hmax = 0.1))
    
    r_end <- slice_max(output, time)[,2]
      
    resid_df$resid[i] <- r_end - data$amt_rem[i]
    }
```


```{r, plot final check}
resid_df %>% ggplot(., aes(x=resid, fill = data$treatment_ID)) + geom_histogram()
```




Model results and runtimes
```{r, results}
summary(readRDS(here("mle","m0_f_fit.rds"))) #no size correction
summary(readRDS(here("mle","m1_f_fit.rds"))) #with size correction
summary(readRDS(here("mle","m2_f_fit.rds"))) #temperature-dependent
summary(readRDS(here("mle","m3_f_fit.rds"))) #resource-dependent
summary(readRDS(here("mle","m4_f_fit.rds"))) #temperature and resource-dependent
summary(readRDS(here("mle","m4_f_fita.rds"))) #temperature and resource-dependent
```


```{r, AIC}
m0 <- readRDS(here("mle","m0_f_fit.rds"))
m1 <- readRDS(here("mle","m1_f_fit.rds"))
m2 <- readRDS(here("mle","m2_f_fit.rds"))
m3 <- readRDS(here("mle","m3_f_fit.rds"))
m4 <- readRDS(here("mle","m4_f_fit.rds"))
```

```{r}
getAIC <- function(n, L){
  return(2*length(n)-(2*L))
}
```


```{r}
getAIC(m0@coef, m0@min)
getAIC(m1@coef, m1@min)
getAIC(m2@coef, m2@min)
getAIC(m3@coef, m3@min)
getAIC(m4@coef, m4@min)
```




